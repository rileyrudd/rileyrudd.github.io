[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Riley",
    "section": "",
    "text": "Thanks for stopping by my website! This blog is all about sharing experiences, insights, and everything in-between. My interests are all over the place, so this is an opportunity for me to present myself in a more thorough way than a resume, while also sharing some detailed experiences/knowledge that may be helpful for others. If you’d like to see my resume, I’ve linked it at the top of my page - otherwise, let’s dive into some details about myself.\nHow to reach me: Feel free to drop me an email at riley.rudd1@gmail.com, or check out my linkedin profile.\n\nThe Journey So Far:\nLet’s jump into my academic background and a few life tidbits:\nMaster of Engineering in Computer Science - Data Analytics & AI: Currently, I’m continuing my academic adventure at Virginia Tech, pursuing a graduate degree in Data Analytics and Artificial Intelligence. It’s been really interesting experience, and I can’t wait to share some of the cool stuff I’ve been learning with you. My expected graduation date is December 2024.\nBachelor of Arts in Economics - I earned my Bachelor of Arts in Economics, magna cum laude, from Virginia Tech in May 2023. I originally didn’t expect to enjoy learning about economics, but I had a great professor during a Principles of Economics class that inspired my interests in economics.\nBachelor of Science in Consumer Studies - Financial Services & Counseling - Fortunately, I had the room in my schedule to fit in a double-major in Consumer Studies. This degree was much more applied knowledge, and led me to becoming President (and Founder) of the Consumer Studies Student Organization. Specializing in financial services and counseling adds a whole new dimension to my life and education. I was also awarded the 2023 Outstanding Senior award for this major.\nMy areas of study may seem a little scattered, but actually, having a broader background has helped me in every professional experience I’ve had. My master’s degree has helped me build immense technical skill in computer science while still having applied understanding of business and government through Consumer Studies, all wrapped up with the theoretical understanding of economics.\n\n\nBeyond Textbooks:\nBesides the academic journey, I’ve had a blast doing some other cool stuff:\nFounder and President of the Virginia Tech Consumer Studies Organization: I worked with faculty members to re-create a student organization inspired by the Consumer Studies major and led the charge from December 2021 to May 2023. We organized meetings, brought in guest speakers, and even had some fun study sessions. One of my favorite memories from this club was our trip to Richmond to visit the Virginia State Corporation Commission, the Attorney General’s Office, and even watched a General Assembly session.\nGetting My Research Hands Dirty: I’ve also dipped my toes into some meaningful research projects. One of them, “Sensing Drought in the Sahel for Household Resilience,” was published by the University of Virginia Biocomplexity Institute. It was a deep dive into real-world climate resilience. This experience was really transformational in my academic career, inspiring me to pursue a Masters degree in Computer Science. I would love future opportunities to use data science for public good.\n\n\nProfessional Adventures:\nMarketing Specialist at TriMech: I’m currently working for TriMech, marketing for our Staffing and Project Engineering teams, juggling various marketing tactics to generate leads and make an impact. I feel a lot of trust with my team at TriMech, and it’s allowed me to build confidence in a wide variety of skills. Our video marketing project won the 2022 SolidWorks Reseller “Best Digital Media Adopt” award. Plus, I’m super proud to be an “Impact Player” as part of the “President’s Club” in 2023.\nResearch Assistant: My work as a Research Assistant was all about working with data to help the U.S. Crop Insurance Program make better decisions. We were digging into drought indicators, focusing on how various approaches would impact payout functions. I worked with one of my favorite Virginia Tech faculty members, Elinor Benami, on this project. She is a great mentor and has inspired me to be a better leader.\nYoung Scholars Research Intern: I worked on the “Data Science for the Public Good” project, using data analysis skills to inform financial donor decisions. This program was really my introduction to data science, and I am so grateful to have been a part of it. I am still close with some of the other interns to this day!\n\n\nBeyond the Resume:\nAside from my work and studies, I’m an avid reader of many topics, but mostly enjoy reading as an escape of reality when needed. To balance out all that brainwork, I’ve also been playing rugby since 2019.\nSo, that’s me in a nutshell. If you’d like to see more of my specific insights, check out some of my blog posts. Don’t be shy – reach out with your questions or thoughts. I’d love to connect with you!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Understanding the Foundations: Probability Theory and Random Variables in Machine Learning",
    "section": "",
    "text": "Probability theory is the mathematical study of uncertainty, quantifying uncertainty about the world. It is important to understand as a basis to machine learning, because the design of learning algorithms often relies on probabilistic assumptions about the data. Probability theory allows algorithms to reason effectively in situations where there cannot be certainty.\n\nRandom Variables\nRandom variables are at the core of probability theory, they are a variable whose value is determined by the outcome of a random process. They can be discrete, taking a specific value from a finite set, or continuous, having any value in a range. In Machine learning, random variables often represent the input features, output predictions, or latent variables in the model.\n\n\nMultiple views on Probability Theory\nWe can divide probability theory into two main branches:\n\nClassical probability: based on equally likely outcomes in a sample space. As an example, when flipping a coin, the probability of getting either heads or tails is 1/2\nBayesian probability: The Bayesian view of probability contrasts frequentist probability in the sense that the latter relies on the existence of one best combination of parameters. The Bayesian probability treats parameters as random variables. Therefore, each parameter has its own probability distribution.\n\n\n\nApplications in Machine Learning\n\nData Modeling: machine learning models often work by using data that has randomness/uncertainty. Random variables are used to quantify this uncertainty, allowing for the development of models.\nBayesian inference: Bayesian methods are used in machine learning for decision-making involving uncertainty. These methods leverage probability theory and random variables to update predictions when new data is available.\nClassification: in classification models, where we assign data to specific categories, random variables can be used to represent the uncertainty with each class assignment.\nReinforcement learning: In reinforcement learning, an agent interacts with an environment to learn optimal actions – random variables are used to represent rewards and consequences, helping the user to make decisions under uncertain circumstances.\n\n\n\nProbability Theory in Action\nLet’s check out a hands-on example using a Gaussian Naive Bayes classifier. This probabilistic model is used for classification tasks and is based on Bayes’ theorem. This algorithm does well in predicting the correct class the features belong to, it considers the probability of occurrence of each class and assigns the label value to the class with the higher probability.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# Generate a synthetic dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n\n# Train a Gaussian Naive Bayes classifier\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'The Accuracy of Prediction is: {accuracy:.2f}')\n\nThe Accuracy of Prediction is: 0.85\n\n\nIn this code chunk, we’ve generated a synthetic data set, split it into a training and testing set, then trained a Gaussian Naive Bayes classifier instance on the data, and printing the accuracy of prediction.\n\n\nVisualizing Probabilistic Predictions\nNow, let’s visualize the probabilistic natures of the Gaussian Naive Bayes predictions using a probability density function (PDF) plot.\n\n# Visualize decision regions and probabilities\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\nprobs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n\nplt.contourf(xx, yy, probs, cmap=\"bwr\", alpha=0.8)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"bwr\", edgecolors=\"k\", marker=\"o\", s=80, linewidth=1)\n\nplt.title(\"Gaussian Naive Bayes: Probability Density Function\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\nIn this visualization, decision regions are plotted based on probabilistic predictions of the model. The scatter plot shows the test set points, color-coded by true class labels.\n\n\nConclusion\nProbability theory is not just a theoretical concept; it forms the backbone of practical machine learning applications. By understanding and leveraging probability theory, we can build models that provide not only predictions but also quantify the uncertainty associated with those predictions. The Gaussian Naive Bayes example showcased here is just one instance of how probability theory seamlessly integrates into machine learning workflows, allowing for robust and informed decision-making."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Riley",
    "section": "",
    "text": "Hey there, thanks for stopping by my blog! I’m Riley Rudd, and I’m excited to have you here. This blog is all about sharing experiences, insights, and everything in-between. My interests are all over the place, so this is an opportunity for me to present myself in a more thorough way than a resume, while also sharing some detailed experiences/knowledge that may be helpful for others. If you’d like to see my resume, I’ve linked it at the top of my page - otherwise, let’s skip the formalities and get to know me a bit better.\nHow to reach me: Feel free to drop me an email at riley.rudd1@gmail.com, or check out my linkedin profile.\nThe Journey So Far:\nLet’s jump into my academic background and a few life tidbits:\nMaster of Engineering in Computer Science - Data Analytics & AI: Currently, I’m continuing my academic adventure at Virginia Tech, pursuing a graduate degree in Data Analytics and Artificial Intelligence. It’s been really interesting experience, and I can’t wait to share some of the cool stuff I’ve been learning with you. My expected graduation date? December 2024.\nBachelor of Arts in Economics: I earned my Bachelor of Arts in Economics from Virginia Tech in May 2023. I originally didn’t expect to enjoy learning about economics, but I had a great professor during a Principles of Economics class that inspired my interests in economics.\nBachelor of Science in Consumer Studies - Financial Services & Counseling: Fortunately, I had the room in my schedule to fit in a double-major in Consumer Studies. This degree was much more applied knowledge, and led me to becoming President (and Founder) of the Consumer Studies Student Organization. Specializing in financial services and counseling adds a whole new dimension to my life and education.\nMy areas of study may seem a little random, but actually, having a broader background has helped me in every professional experience I’ve had. My master’s degree has helped me build immense technical skill in computer science while still having applied understanding of business and government through Consumer Studies, all wrapped up with the theoretical understanding of economics.\nBeyond Textbooks:\nBesides the academic journey, I’ve had a blast doing some other cool stuff:\nFounder and President of the Virginia Tech Consumer Studies Organization: I worked with faculty members to re-create a student organization inspired by the Consumer Studies major and led the charge from December 2021 to May 2023. We organized meetings, brought in guest speakers, and even had some fun study sessions. One of my favorite memories from this club was our trip to Richmond to visit the Virginia State Corporation Commission, the Attorney General’s Office, and even watched a General Assembly session.\nGetting My Research Hands Dirty: I’ve also dipped my toes into some meaningful research projects. One of them, “Sensing Drought in the Sahel for Household Resilience,” was published by the University of Virginia Biocomplexity Institute. It was a deep dive into real-world climate resilience. This experience was really transformational in my academic career, inspiring me to pursue a Masters degree in Computer Science. I would love future opportunities to use data science for public good.\nProfessional Adventures:\nMarketing Specialist at TriMech: I’m currently working for TriMech, marketing for our Staffing and Project Engineering teams, juggling various marketing tactics to generate leads and make an impact. I feel a lot of trust with my team at TriMech, and it’s allowed me to build confidence in a wide variety of skills. Our video marketing project won the 2022 SolidWorks Reseller “Best Digital Media Adopt” award. Plus, I’m super proud to be an “Impact Player” as part of the “President’s Club” in 2023.\nResearch Assistant: My work as a Research Assistant was all about working with data to help the U.S. Crop Insurance Program make better decisions. We were digging into drought indicators, focusing on how various approaches would impact payout functions. I worked with one of my favorite Virginia Tech faculty members, Elinor Benami, on this project. She is a great mentor and has inspired me to be a better leader.\nYoung Scholars Research Intern: I worked on the “Data Science for the Public Good” project, using data analysis skills to inform financial donor decisions. This program was really my introduction to data science, and I am so grateful to have been a part of it. I am still close with some of the other interns to this day!\nBeyond the Resume:\nAside from my work and studies, I’m an avid reader of many topics, but mostly enjoy reading as an escape of reality when needed. To balance out all that brainwork, I’ve also been playing rugby since 2019.\nSo, that’s me in a nutshell. If you’d like to see more of my specific insights, check out some of my blog posts. Don’t be shy – reach out with your questions or thoughts. I’d love to connect with you!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Understanding Classification Metrics: ROC, PR, and Confusion Matrix\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nRiley Rudd\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression in Machine Learning: Finding Predictive Relationships\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nRiley Rudd\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Power of Clustering in Machine Learning\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nRiley Rudd\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the Foundations: Probability Theory and Random Variables in Machine Learning\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nRiley Rudd\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "Unveiling the Power of Clustering in Machine Learning",
    "section": "",
    "text": "Clustering is a type of unsupervised learning method, meaning conclusions are drawn about datasets without labeled responses. It is done by grouping a particular set of points based on their characteristics, aggregating them by their similarities. Clustering is generally used to find meaningful structure, underlying processes, and grouping inherent in a set of examples. Clustering can be thought of like organizing music by genre, something we all do naturally. By using clustering, you can learn something about your data previously unknown or invisible.\n\nWhy Use Clustering?\nReal world examples are very complex and can be difficult to categorize. When you need to answer questions about this data, it is necessary to understand patterns within your data. Some patterns are easily visible to the eye, such as groups of height and weight points on a x and y axis. However, when there are many dimensions, it is impossible to visualize this on a 2D scale and therefore not possible to see by eye. Clustering can be useful in many industries. A few common examples include: anomaly detection, medical imaging, and market segmentation. Consider the example of using clustering for data compression. Replacing feature data for cluster ID will simplify the dataset and saves storage – which is a significant benefit when considering large datasets. This allows for faster and simpler training of machine learning models.\n\n\nClustering Methods\n\nDensity-Based methods: these methods consider the clusters as the dense region having some similarities and differences from lower dense region of the space. They have good accuracy and ability to merge clusters. One example is the Density-Based Spatial Clustering of Applications with Noise (DBScan). It is effective in identifying clusters of various shapes and sizes.\nK-Means Clustering: this is one of the most popular clustering algorithms. It partitions the dataset into “k” clusters based on the mean values of data points. It iteratively assigns data points to clusters and recalculates the cluster until it converges.\nMean Shift: this is a non-parametric clustering algorithm that identifies dense regions in the data distribution. It iteratively shifts the center of cluster towards the modes of the data distribution.\nGaussian Mixture Models (GMM): These algorithms assumes the data is generated from a mixture of several Gaussian distributions, and models each cluster as a Gaussian distribution, assigning probabilities to data points belonging to each cluster.\n\n\n\nChallenges with Clustering\n\nChoosing the Right Number of Clusters: selecting the appropriate number of clusters can be challenging and considerably impact the results. There are analysis methods to help determine the optimal “k” clusters.\nSensitivity to Initial Conditions: some algorithms are sensitive to initial placement of centroids, requiring multiple initializations to obtain stable results.\nHandling High-Dimensional Data: clustering high dimensional data can be complex, requiring dimensionality reduction techniques to improve the algorithm’s performance.\n\n\n\nHow is Clustering Achieved with DBSCAN?\nTo start, features in the dataset must all be numerically represented. If one of your features includes a “review” such as with ecommerce sites, you would need to define that review numerically, such as representing positive reviews with 0s and negative reviews with 1s. Then, the user will define a few parameters. One goal of clustering is to minimize inter-cluster similarity. This means that points in two different clusters should be as far apart (eg. different) as possible. The most important is determining the maximum distance between two points for one to be considered in the neighborhood of another. Another parameter is the minimum number of points in a neighborhood for a point to be considered a core point. If you choose a higher number, the resulting clusters will be more dense, and lower numbers result in clusters that are more sparse.\nDBSCAN will then begin with a point to determine where the core points are, adding each core point into a cluster if they are in the same neighborhood. Then, it will add the non-core points, which are points in the same neighborhood as core points, but do not have enough members in the neighborhood to further extend the cluster. DBSCAN moves sequentially through each cluster until all points fall into a cluster or an outlier.\n\n\nDeeper Dive into Clustering\nWe can begin by creating a synthetic dataset that will look like 3 distinct blobs.\n\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\n#generate syntetic clusters\ncenters = [[1,1], [-1,-2], [2,3]]\nX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)\nX = StandardScaler().fit_transform(X)\n\n#visualizing data\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\nNext, we will compute DBSCAN, printing the number of clusters and noise points, and finally visualizing where DBSCAN has identified clusters. In the plot, core points are larger than non-core points and color coded according to cluster. Noise samples are in black.\n\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.cluster import DBSCAN\n\n# computer DBSCAN\ndb = DBSCAN(eps=0.2, min_samples=10).fit(X)\nlabels = db.labels_\n\n# see how many clusters and noise points\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\nprint(\"Number of clusters: %d\" % n_clusters)\nprint(\"Number of noise points: %d\" % n_noise)\n\n#visualize clusters \nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"Estimated number of clusters: {n_clusters}\")\nplt.show()\n\nNumber of clusters: 3\nNumber of noise points: 17\n\n\n\n\n\nIn this example, the dataset is hypothetically assumed to be about geographical music preferences. If it were a real data set, it may have been geographical locations with other features such as tempo, energy level, and beat pattern. DBSCAN identifies clusters of data points that are dense and well connected. The parameters for DBSCAN are “eps” which defines the maximum distance between samples to be considered near another, and “min_samples” which sets the number of samples in a neighborhood for a data point to be considered a core point. Next, DBSCAN will begin with an arbitrary point and expand the cluster by adding nearby points if they are densely packed. It will continue until no more points can be added, therefore having formed the cluster. After this, DBSCAN assigns the cluster label to each point. Those outside of any cluster are labeled as noise or outliers.\n\n\nReferences\n\nDemo of DBSCAN clustering algorithm — scikit-learn 1.3.2 documentation"
  },
  {
    "objectID": "posts/post3/index.html",
    "href": "posts/post3/index.html",
    "title": "Linear Regression in Machine Learning: Finding Predictive Relationships",
    "section": "",
    "text": "Linear regression, a statistical model, serves as a fundamental tool for comprehending the correlation between input and output numerical variables. This model becomes particularly relevant in machine learning scenarios where the aim is to predict one variable based on another. The core idea behind linear regression is to identify a linear relationship, essentially determining the slope of the line and the y-intercept.\n\nModel Training and Fitting\nTo initiate the process, a training dataset comprising samples is employed. While it’s possible to manually calculate the linear regression equation, modern tools like scikit-learn's LinearRegression model offer efficient solutions. After fitting the model to the data, predicting the position of a new data point becomes a straightforward task using the predict function.\n\n\nBenefits: Extension, Interpretability, Insight\nLinear regression is not confined to single-variable scenarios. It seamlessly extends to accommodate datasets with multiple dimensions, providing a versatile solution for a variety of prediction tasks. One of the remarkable features of linear regression is its interpretability. A positive slope clearly indicates that as the input variable (x) increases, the output variable (y) also increases, providing straightforward insights into the relationship.\n\n\nModel Complexity and Coefficients:\nThe complexity of a regression model is characterized by the number of coefficients it employs. A coefficient of zero signifies no influence of the corresponding input variable on the model, adding a layer of interpretability to the model.\n\n\nGradient Descent for Model Fitting:\nUnderstanding how a machine learning model achieves a good fit involves the concept of Gradient Descent. This iterative process begins with random values for each coefficient. By calculating the sum of squared errors for each pair of inputs and outputs, the model’s coefficient predictions are updated to minimize the error. This cycle repeats until further improvement is deemed impossible.\n\n\nData Preparation for Optimal Results:\nWhile not mandatory, adhering to certain guidelines enhances the performance of linear regression models. Ensuring a linear relationship in the data is crucial, sometimes requiring transformations like log transformations for exponential relationships. Additionally, the assumption of non-noisy input and output variables underscores the importance of cleaning data by removing outliers before model fitting.\nIn conclusion, linear regression stands as a powerful and interpret-able tool in the realm of machine learning, providing valuable insights into predictive relationships between variables.\n\n\nVisualizing Linear Regression\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)  # Generate 100 random values\ny = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with some noise\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Visualize the results\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.plot(X_test, y_pred, color='blue', linewidth=3, label='Linear Regression Model')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Demo')\nplt.legend()\nplt.show()\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n\n\n\nMean Squared Error: 0.6536995137170021"
  },
  {
    "objectID": "posts/post4/index.html",
    "href": "posts/post4/index.html",
    "title": "Understanding Classification Metrics: ROC, PR, and Confusion Matrix",
    "section": "",
    "text": "Classification is a supervised, meaning labels are given, learning method where a model attempts to predict the correct label of a given input data sample. The models are trained using training data, and then is evaluated and corrected before being used to make predictions on new data. For example, a classification model could be used to detect whether a tumor is cancerous or not based on input data.\n\nTypes of Classification Tasks in Machine Learning\n\nBinary classification: The goal here is to determine whether input data is one or another categories. As an example, a model may want to determine whether an email is either spam or not spam.\nMulti-class classification: With this category, the goal is to determine what the input data is between at least two, but possibly more, categories. An example could be determining what category a flower falls into between various types of flowers.\nMulti-label classification: These tasks attempt to predict 0 or more classes for each input. An input can have more than one label.\n\n\n\nIssues Faced in Classification\n\nImbalanced classification: This issue is faced when the training data is unevenly distributed in each class. Certain models are better at handling imbalanced datasets than others, as they are less biased toward predicting classes with highest number of observations.\nOver and Under Fitting: overfitting occurs when the model learns the training data too well, gathering noise and performs poorly when given new data. However, underfitting models are too simple and fail to capture underlying patterns in data.\nNoise in Data: Noisy data, with errors or irrelevant information can impact the performance of a classification model. Cleaning the data properly is important to avoid this.\nHigh Dimensionality: Too many features in a dataset causes a model's performance to degrade due to the curse of dimensionality. There are techniques, like principal component analysis, which can help to reduce dimensions while retaining underlying patterns.\n\n\n\nTypes of Classification in Machine Learning\n\nLazy Learners: these models store training data and wait for testing data to appear – when it does, classification is conducted based on most related training data. More time is spent on predicting than training. An example of a lazy learner algorithm is the K-nearest neighbor algorithm.\nEager Learners: These models construct a classification model on given training data before testing data is entered. These models take more time to train and less time to predict.\n\n\n\nEvaluating Performance\n\nReceiver Operating Characteristic (ROC) curve: This is a graphical representation of a classifier's ability to distinguish between classes by varying the decision threshold. It plots the sensitivity against the false positive rate across various threshold values. Classifiers with higher area under the curve are deemed more effective.\nPrecision Recall (PR) Curve: Valuable in imbalanced datasets, the PR curve focuses on the trade-off between precision and recall. It illustrates how well a classifier can identify positive instances while maintaining precision. The Area Under the PR Curve (AUC-PR) provides a comprehensive metric for evaluating performance.\nConfusion matrix: The confusion matrix offers a breakdown of a classifier’s performance, categorizing predictions into true positives, true negatives, false positives, and false negatives. From the confusion matrix, essential metrics like accuracy, precision, recall, and the F1 score can be derived, providing a nuanced understanding of the model’s strengths and weaknesses.\n\n\n\nClassification Demonstration\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\nimport matplotlib.pyplot as plt\n\n#generate synthetic data, features, and labels (binary)\nnp.random.seed(42)\nX = np.random.rand(1000, 5)\ny = np.random.randint(2, size=1000)\n\n#train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Using a Random Forest Classifier as an example\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprecision, recall, _ = precision_recall_curve(y_test, y_prob)\npr_auc = auc(recall, precision)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\n\nVisualize Results\n\n# Plot ROC Curve\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\n\n# Plot PR Curve\nplt.subplot(1, 2, 2)\nplt.plot(recall, precision, color='blue', lw=2, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall (Sensitivity)')\nplt.ylabel('Precision')\nplt.title('Precision-Recall (PR) Curve')\nplt.legend(loc='lower left')\n\nplt.tight_layout()\nplt.show()\n\n# Display Confusion Matrix\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n\n\n\nConfusion Matrix:\n[[37 62]\n [45 56]]"
  }
]