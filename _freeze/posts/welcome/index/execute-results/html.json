{
  "hash": "f0c4e4708d52cb0c79d43c6564f75df2",
  "result": {
    "markdown": "---\ntitle: \"Understanding the Foundations: Probability Theory and Random Variables in Machine Learning\"\nauthor: \"Riley Rudd\"\ndate: \"2023-10-16\"\ncategories: [machine learning]\n---\n\nProbability theory is the mathematical study of uncertainty, quantifying uncertainty about the world. It is important to understand as a basis to machine learning, because the design of learning algorithms often relies on probabilistic assumptions about the data. Probability theory allows algorithms to reason effectively in situations where there cannot be certainty.\n\n### Random Variables\n\nRandom variables are at the core of probability theory, they are a variable whose value is determined by the outcome of a random process. They can be discrete, taking a specific value from a finite set, or continuous, having any value in a range. In Machine learning, random variables often represent the input features, output predictions, or latent variables in the model.\n\n### Multiple views on Probability Theory\n\nWe can divide probability theory into two main branches:\n\n-   Classical probability: based on equally likely outcomes in a sample space. As an example, when flipping a coin, the probability of getting either heads or tails is 1/2\n\n-   Bayesian probability: The Bayesian view of probability contrasts frequentist probability in the sense that the latter relies on the existence of one best combination of parameters. The Bayesian probability treats parameters as random variables. Therefore, each parameter has its own probability distribution.\n\n### Applications in Machine Learning\n\n-   Data Modeling: machine learning models often work by using data that has randomness/uncertainty. Random variables are used to quantify this uncertainty, allowing for the development of models.\n\n-   Bayesian inference: Bayesian methods are used in machine learning for decision-making involving uncertainty. These methods leverage probability theory and random variables to update predictions when new data is available.\n\n-   Classification: in classification models, where we assign data to specific categories, random variables can be used to represent the uncertainty with each class assignment.\n\n-   Reinforcement learning: In reinforcement learning, an agent interacts with an environment to learn optimal actions -- random variables are used to represent rewards and consequences, helping the user to make decisions under uncertain circumstances.\n\n### Probability Theory in Action\n\nLet's check out a hands-on example using a Gaussian Naive Bayes classifier. This probabilistic model is used for classification tasks and is based on Bayes' theorem. This algorithm does well in predicting the correct class the features belong to, it considers the probability of occurrence of each class and assigns the label value to the class with the higher probability.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# Generate a synthetic dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n\n# Train a Gaussian Naive Bayes classifier\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'The Accuracy of Prediction is: {accuracy:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Accuracy of Prediction is: 0.85\n```\n:::\n:::\n\n\nIn this code chunk, we've generated a synthetic data set, split it into a training and testing set, then trained a Gaussian Naive Bayes classifier instance on the data, and printing the accuracy of prediction.\n\n### Visualizing Probabilistic Predictions\n\nNow, let's visualize the probabilistic natures of the Gaussian Naive Bayes predictions using a probability density function (PDF) plot.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Visualize decision regions and probabilities\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\nprobs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n\nplt.contourf(xx, yy, probs, cmap=\"bwr\", alpha=0.8)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"bwr\", edgecolors=\"k\", marker=\"o\", s=80, linewidth=1)\n\nplt.title(\"Gaussian Naive Bayes: Probability Density Function\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=587 height=449}\n:::\n:::\n\n\nIn this visualization, decision regions are plotted based on probabilistic predictions of the model. The scatter plot shows the test set points, color-coded by true class labels.\n\n### **Conclusion**\n\nProbability theory is not just a theoretical concept; it forms the backbone of practical machine learning applications. By understanding and leveraging probability theory, we can build models that provide not only predictions but also quantify the uncertainty associated with those predictions. The Gaussian Naive Bayes example showcased here is just one instance of how probability theory seamlessly integrates into machine learning workflows, allowing for robust and informed decision-making.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}